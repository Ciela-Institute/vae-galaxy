{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48447ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21a372ca690>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "from torch.utils.data import DataLoader \n",
    "import matplotlib.pyplot as plt\n",
    "import h5py \n",
    "import sys\n",
    "models_path = \"..\\\\Models architecture\"\n",
    "sys.path.append(models_path)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b78d128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4b3338",
   "metadata": {},
   "source": [
    "# Importing galaxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb4a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cosmic_survey = 'C:\\\\Users\\\\noedi\\\\Desktop\\\\Stage été 2022\\\\Code stage\\\\Vae Galaxies\\\\Data\\\\cosmos_25.2_all_with_zphot.h5'\n",
    "path_deepfield = 'C:\\\\Users\\\\noedi\\\\Desktop\\\\Stage été 2022\\\\Code stage\\\\Vae Galaxies\\\\Data\\\\cosmos_23.5_128_augmented_denoised.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df9389ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hdf_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.hdf = h5py.File(file, 'r')\n",
    "        self.datasets = list(self.hdf.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        size = len(self.hdf[self.datasets[0]])\n",
    "        return size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        if len(self.datasets) == 2:\n",
    "            x = self.hdf[self.datasets[0]][idx]\n",
    "            z = self.hdf[self.datasets[1]][idx]\n",
    "            return x, z\n",
    "        \n",
    "        else:\n",
    "            x = self.hdf[self.datasets[0]][idx]\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb36f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmic_survey_dataset = Hdf_dataset(path_cosmic_survey)\n",
    "deepfield_dataset = Hdf_dataset(path_deepfield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "041f9c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change cosmic_survey_dataset by deepfield_dataset if you want to train with Model_128vae \n",
    "data_size = len(cosmic_survey_dataset) # <-- change dataset if needed ! \n",
    "train_size = int(0.9 * data_size)\n",
    "val_size = data_size - train_size\n",
    "\n",
    "Generator = torch.Generator()\n",
    "Generator.manual_seed(0)\n",
    "train_set, val_set = torch.utils.data.random_split(cosmic_survey_dataset, [train_size, val_size], generator = Generator) # <-- change dataset if needed ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd73e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 64\n",
    "train_loader = DataLoader(train_set, batch_size = batchsize)\n",
    "val_loader = DataLoader(val_set, batch_size = batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9eefc7",
   "metadata": {},
   "source": [
    "# Training Time\n",
    "To train, `vae.train_time(train_loader, val_loader, epochs = 100, learning_rate = 1e-3 , beta = 0.1)`.  \n",
    "$\\beta$ can be a scalar or a table of values (list, array, tensor). Just make sure `len(beta) = epochs`.  \n",
    "If you are working with the _fancy_cvae_ architecture `vae.train_time(*args, k=1000)` has an extra hyperparameter $k$ which is just a multiplier of the loss function between the true redshift value and its prediction (which is part of the general loss). Hence, with a greater value for $k$ neural network will learn better predictions of the redshifts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e52ee33-cb87-4cd8-a1bf-ce9491ec608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the model: Model_128vae, Model_158vae, cvae, cvae2, fancy_cvae\n",
    "from cvae2 import VariationalAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5037c7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b0e243f2254fc8ae886944b4bc5bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\noedi\\Desktop\\Stage été 2022\\Code stage\\Vae Galaxies\\vae-project\\notebooks\\Training.ipynb Cellule 11\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/noedi/Desktop/Stage%20%C3%A9t%C3%A9%202022/Code%20stage/Vae%20Galaxies/vae-project/notebooks/Training.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m Beta \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m# Beta-vae \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/noedi/Desktop/Stage%20%C3%A9t%C3%A9%202022/Code%20stage/Vae%20Galaxies/vae-project/notebooks/Training.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m vae\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/noedi/Desktop/Stage%20%C3%A9t%C3%A9%202022/Code%20stage/Vae%20Galaxies/vae-project/notebooks/Training.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train_loss, val_loss, mse, kl \u001b[39m=\u001b[39m vae\u001b[39m.\u001b[39;49mtrain_time(train_loader, val_loader, epochs \u001b[39m=\u001b[39;49m num_epochs, learning_rate \u001b[39m=\u001b[39;49m lr, beta \u001b[39m=\u001b[39;49m Beta)\n",
      "File \u001b[1;32mc:\\Users\\noedi\\Desktop\\Stage été 2022\\Code stage\\Vae Galaxies\\vae-project\\notebooks\\..\\Models architecture\\cvae2.py:165\u001b[0m, in \u001b[0;36mVariationalAutoencoder.train_time\u001b[1;34m(self, train_loader, val_loader, epochs, learning_rate, beta)\u001b[0m\n\u001b[0;32m    163\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m    164\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m--> 165\u001b[0m train_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39;49mitem())\n\u001b[0;32m    166\u001b[0m mse\u001b[39m.\u001b[39mappend(loss_fn(x_pred, x)\u001b[39m.\u001b[39mitem())\n\u001b[0;32m    167\u001b[0m kl\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mkl\u001b[39m.\u001b[39mitem())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Network parameters (for cvae2, nc= number of channels = 1, for cvae and fancy_cvae, nc = number of channels= 2)\n",
    "z_dim =  32\n",
    "vae = VariationalAutoencoder(z_dim).to(device)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100\n",
    "lr = 1e-4\n",
    "Beta = 1 # Beta-vae \n",
    "vae.train()\n",
    "train_loss, val_loss, mse, kl = vae.train_time(train_loader, val_loader, epochs = num_epochs, learning_rate = lr, beta = Beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929de2fd",
   "metadata": {},
   "source": [
    "# Saving the weights and the loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101def74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically creates the name of the file \n",
    "hyperparameters = {'batch size': batchsize, 'epochs': num_epochs, 'beta': Beta, 'learning rate': lr, 'z_dim': z_dim}\n",
    "\n",
    "if type(Beta) == torch.Tensor or type(Beta) == np.ndarray or type(Beta) == list:\n",
    "    beta_behaviour = input(\"Enter a word to describe beta's behaviour\").replace(\" \",\"\")\n",
    "    loss_file_name = \"z\"+str(z_dim)+\"_beta\"+beta_behaviour+\"_loss.pt\"\n",
    "    weights_file_name = \"z\"+str(z_dim)+\"_beta\"+beta_behaviour+\"_weights.pt\"\n",
    "else: \n",
    "    loss_file_name = \"z\"+str(z_dim)+\"_beta\"+str(Beta)+\"_loss.pt\"\n",
    "    weights_file_name = \"z\"+str(z_dim)+\"_beta\"+str(Beta)+\"_weights.pt\"\n",
    "\n",
    "data_training_cosmic_survey = \"..\\\\Data\\\\\" \n",
    "data_training_deepfield = \"..\\\\Data\\\\\"\n",
    "\n",
    "weights_path = data_training_cosmic_survey + weights_file_name\n",
    "loss_path = data_training_cosmic_survey + loss_file_name\n",
    "\n",
    "# torch.save([vae.state_dict(), hyperparameters], weights_path)\n",
    "# torch.save([train_loss, val_loss, mse, kl, hyperparameters], loss_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d7fd2a48356971e58618481ace9fdf054dd5c32337f32d6ebacd58cdfb77420"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
